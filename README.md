# metric_interpretation
 This repository contains the code used for our simulations to capture the variability in classification metrics in the presence of class imbalance and when a random or proportional prediction approach is used. In essence, we are trying to provide practitioners for a benchmark of the performance of a dummy model (when containing no predictors) in order to assess how much improvement is their trained model (and/or their predictors) are adding to the results
