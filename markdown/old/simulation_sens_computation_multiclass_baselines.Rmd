---
title: "A Numerical Study to Examine the Obtained Sensitivity for Arbitrary and Proportional Guessing Scenarios for a 4-Class Classification Problem"
author:
  - name: "Fadel M. Megahed ^[Email: fmegahed@miamioh.edu | Phone: +1-513-529-4185 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/megahefm\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
  - name: "Allison Jones-Farmer ^[Email: farmerl2@miamioh.edu | Phone: +1-513-529-4823 | Website: <a href=\"https://miamioh.edu/fsb/directory/?up=/directory/farmerl2\">Miami University Official</a>]"
    affiliation: Farmer School of Business, Miami University
  - name: "Steve Rigdon ^[Email: steve.rigdon@slu.edu | Website: <a href=\"https://www.slu.edu/public-health-social-justice/faculty/rigdon-steven.php\">Saint Louis University Official</a>]"
    affiliation: College of  Public Health and Social Justice, Saint Louis University
csl: apa.csl
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    css: custom.css
    code_folding: show
    code_download: TRUE
    number_sections: TRUE
    paged_df: TRUE
    toc: TRUE
    toc_float: TRUE
    theme: readable
  includes:
    in_header: structure.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      progress = FALSE, 
                      verbose = FALSE,
                      dev = c('png'),
                      fig.retina = 2,
                      out.width = '100%',
                      fig.asp = 0.7)

options(qwraps2_markup = "markdown")

# Setting the random seed and chunk dependencies
knitr::opts_chunk$set(cache.extra = set.seed(2022),
                      autodep = TRUE) 
knitr::dep_auto()
```


# R Setup and Required Packages
In this project, the open-source R programming language is used to contrast the sensitivities obtained from three baseline modeling scenarios with that obtained from the multinomial regression model in [Megahed et al. 2021](https://fmegahed.github.io/covid_deaths.html). 

R is maintained by an international team of developers who make the language available at [The Comprehensive R Archive Network](https://cran.r-project.org/). Readers interested in reusing our code and reproducing our results should have R installed locally on their machines. R can be installed on a number of different operating systems (see [Windows](https://cran.r-project.org/bin/windows/), [Mac](https://cran.r-project.org/bin/macosx/), and [Linux](https://cran.r-project.org/bin/linux/) for the installation instructions for these systems). We also recommend using the RStudio interface for R. The reader can [download RStudio](http://www.rstudio.com/ide) for free by following the instructions at the link. For non-R users, we recommend the [Hands-on Programming with R](https://rstudio-education.github.io/hopr/packages.html) for a brief overview of the software's functionality. Hereafter, we assume that the reader has an introductory understanding of the R programming language.

In the code chunk below, we load the packages used to support our analysis. Note that the code of this and any of the code chunks can be hidden by clicking on the 'Hide' button to facilitate the navigation. **The reader can hide all code and/or download the Rmd file associated with this document by clicking on the Code button on the top right corner of this document.**


```{r packages, cache=FALSE}
if(require(pacman)==FALSE) install.packages("pacman")
pacman::p_load(tidyverse, caret, broom, ggtext)
```

---

# Scenario 1: A Balanced Die

The first scenario represents the simplest of baseline models, where both the actuals/observations and the predictions are generated by randomly sampling with replacement a vector of 3,108 from four classes. The size of the vector was set to equal to the number of continguous U.S. counties in our original paper. This process was repeated 10,000 to capture the variability in the sensitivity performance when such a model is used on a dataset of 3,108 observations.

```{r 4_sided_die_balanced}
num_sim = 10^4

results_bal_die = tibble()

for (i in 1:num_sim) {
  sim_data = data.frame(
    obs = sample( x = c( paste0('c', 1:4)), # four classes
                  size = 3108, # size of sample = number of continguous counties
                  replace = T) %>%  # sample with replacement
      as.factor(),
    pred = sample( x = c( paste0('c', 1:4)), # four classes
                   3108, # size of sample = number of continguous counties
                   replace = T) %>%  # sample with replacement
      as.factor()
  )
  
  results_bal_die = bind_rows(
    results_bal_die, 
    # compute confusion matrix based on the caret package
    confusionMatrix(
      data = sim_data$pred, reference = sim_data$obs) %>% 
      # tidy the results and focus on the byClass results
      broom::tidy(byClass = T) %>% 
      # select the five measures of interest (where probs were not computed)
      filter(term %in% c('accuracy', 'sensitivity',
                         'specificity', 'f1')) %>% 
      # adding a column titled scenario
      mutate(sim_num = i,
             scenario = 'die_balanced_obs') %>%
      # keeping only 5 columns since conf.intervals were only computed for acc
      select(sim_num, scenario, term, class, estimate)
  )
}


```

---

# Scenario 2: A Balanced Die with Actuals Generated based on our Imbalanced Data

Here, we generate the actuals based on how the 3108 counties were distributed according to the cluster assignment in [our original analysis for COVID-19 deaths](https://fmegahed.github.io/covid_deaths.html). 

```{r imbalanced_die}
results_imbal_die = tibble()

for (i in 1:num_sim) {
  sim_data = data.frame(
    obs = sample( x = c( rep('c1', 1261),
                         rep('c2', 226),
                         rep('c3', 827),
                         rep('c4', 794)), # four classes
                  size = 3108,  # size = number of continguous counties
                  replace = T) %>%  # sample with replacement
      as.factor(),
    pred = sample( x = c( paste0('c', 1:4)), # four classes
                   size = 3108, # size = number of continguous counties
                   replace = T) %>%  # sample with replacement
      as.factor()
  )
  
  results_imbal_die = bind_rows(
    results_imbal_die, 
    # compute confusion matrix based on the caret package
    confusionMatrix(
      data = sim_data$pred, reference = sim_data$obs) %>% 
      # tidy the results and focus on the byClass results
      broom::tidy(byClass = T) %>% 
      # select the five measures of interest (where probs were not computed)
      filter(term %in% c('accuracy', 'sensitivity',
                         'specificity', 'f1')) %>% 
      # adding a column titled scenario
      mutate(sim_num = i,
             scenario = 'die_imbalanced_obs') %>%
      # keeping only 5 columns since conf.intervals were only computed for acc
      select(sim_num, scenario, term, class, estimate)
  )
}

```

---

# Scenario 3: Proportional Guessing

Here, we assume that the modeler can use a proportional guessing model, where the frequency of predicting the four classes is set to equal to that observed in the training data.


```{r prop_guessing}
results_prop_pred = tibble()

for (i in 1:num_sim) {
  sim_data = data.frame(
    obs = sample( x = c( rep('c1', 1261),
                         rep('c2', 226),
                         rep('c3', 827),
                         rep('c4', 794)), # four classes
                  size = 3108, # size = number of continguous counties
                  replace = T) %>%  # sample with replacement
      as.factor(),
    pred = sample( x = c( rep('c1', 1261),
                          rep('c2', 226),
                          rep('c3', 827),
                          rep('c4', 794)), # four classes
                   size = 3108, # size = number of continguous counties
                   replace = T) %>%  # sample with replacement
      as.factor()
  )
  
  results_prop_pred = bind_rows(
    results_prop_pred, 
    # compute confusion matrix based on the caret package
    confusionMatrix(
      data = sim_data$pred, reference = sim_data$obs) %>% 
      # tidy the results and focus on the byClass results
      broom::tidy(byClass = T) %>% 
      # select the five measures of interest (where probs were not computed)
      filter(term %in% c('accuracy', 'sensitivity',
                         'specificity', 'f1')) %>% 
      # adding a column titled scenario
      mutate(sim_num = i,
             scenario = 'proportional_pred') %>%
      # keeping only 5 columns since conf.intervals were only computed for acc
      select(sim_num, scenario, term, class, estimate)
  )
}

```


---

# Results

## Numerical Summary

```{r results_overall}
results = bind_rows(results_bal_die,
                    results_imbal_die,
                    results_prop_pred)

write_csv(x = results, file = 'sim_results.csv')

results %>% group_by(scenario, class) %>% 
  filter(term == 'sensitivity') %>% 
  summarise(mean(estimate))

```

## Histogram of Sensitivity Distribution for all Scenarios and Classes

```{r plot}
results %>% filter(term == 'sensitivity') %>% 
  mutate(scenario = recode(
    scenario,
    die_balanced_obs = '4-sided die (balanced data)',
    die_imbalanced_obs = '4-sided die (imbalanced data)',
    proportional_pred = 'proportional guessing'),
    reported_values = case_when(
      class == 'c1' ~ 0.7105,
      class == 'c2' ~ 0.4204,
      class == 'c3' ~ 0.3906,
      class == 'c4' ~ 0.7427
    )) %>% 
  ggplot(aes(x = estimate, group = class)) +
  facet_grid(cols = vars(scenario), rows = vars(class), scales = 'fixed') +
  geom_histogram(aes(fill = as.factor(class)), binwidth = 0.01, color = 'black') +
  scale_x_continuous(breaks = scales::pretty_breaks(5), limits = c(0,1)) +
  scale_y_continuous(breaks = scales::pretty_breaks(5), labels = scales::comma) +
  scale_fill_brewer(palette = "Paired") +
  scale_color_brewer(palette = "Paired") +
  geom_vline(aes(xintercept = reported_values), size = 1, color = 'black') +
  # geom_curve(aes(x = reported_values + 0.15, y = 2750, xend = reported_values, yend = 3250), 
  #            colour = "black", 
  #            size=1.5, 
  #            curvature = 0.25,
  #            arrow = arrow(length = unit(0.055, "npc"))) +
  geom_text(aes(x = reported_values + 0.15, y = 2000, 
                label = paste('reported\nsensitivity\nin paper \n for', class)),
            color = 'black', size = 3) +
  theme_bw() +
  theme(legend.position = 'none') +
  labs(x = 'Sensitivity Values', y = 'Count', 
       title = paste('A histogram of sensitivity values based on',
                     scales::comma(num_sim), 'simulation runs'))
```